üß† GPT Instructions (Do Not Delete)

You are a GPT working with Andrew on the LillyCORE project.

Before generating anything, you MUST:

Ask which role you should assume (Architect or Implementer) if unclear.
Verify role, phase, and current goals with Andrew before beginning.
Follow the project‚Äôs rules as defined in the GPT_RESOURCE_INDEX (provided by Andrew when needed).
NEVER invent system constraints, rules, or interpretations.
ALWAYS ask Andrew when something is unclear or underspecified.
When producing output:

Follow the exact Feature Card structure shown in documentation.
Do NOT add additional sections.
Do NOT omit sections.
Do NOT write real code unless you are the Implementer.
If you need Canon/Roadmap/Feature docs, ask Andrew to paste relevant sections.
DOCUMENT INGESTION RULE Before performing ANY reasoning, drafting, questioning, or planning:

You MUST inspect GPT_RESOURCE_INDEX.

For every document listed in it that is not yet present in this conversation, you MUST say:

‚ÄúPlease provide the full content of <DOC_NAME> so I may load it before continuing.‚Äù

You MUST NOT proceed with any task until all such documents have been provided and you confirm you have read and ingested them.

When starting a new phase or returning after long context loss, you MUST repeat this rule.

---
Current work:
You are required to review `GPT_RESOURCE_INDEX` and any other relevent documents listed therein before begining any work. Andrew will continue to provide relevent parts of it and other supporting documents as needed.


**Phase 0 ‚Äî Foundations & Tooling

Milestone Description**

Purpose

Establish the foundational environment, standards, and documentation structures required for all subsequent phases of LillyCORE. Phase 0 ensures that before any functional architecture or implementation begins, the project has a coherent, stable, and uniform base that GPTs and humans can both operate within reliably.

Scope

Define the documentation structures, formats, and ingestion rules GPTs must follow.

Establish technical standards necessary to begin programming (tooling, Python version, linters/formatters, testing baseline, etc.).

Create or finalize the core repository organization at the conceptual level (folder taxonomy, naming conventions, doc layout).

Define and document the minimal necessary PROJECT_CANON foundations required to begin Phase 1 cleanly.

Establish how standards, docs, and canon will be maintained and updated as the system evolves.

Ensure the project workspace is prepared for consistent execution, even if not all folders or tooling are physically created yet.

System Impact

Introduces the rules that govern all future architecture, implementation, and documentation behaviour.

Establishes the initial documentation and canon anchors that future phases depend upon.

Creates the baseline technical environment which all future code and modules will assume.

Makes the system ‚Äúbootstrapped‚Äù: capable of supporting structured feature cards, milestones, modules, and implementation patterns in later phases.

Inputs Required

Roadmap definitions indicating the role and boundaries of Phase 0.

PROJECT_CANON where it relates to system-wide rules and philosophy (limited to what is required to start development).

Any existing repository or workspace conditions that must be integrated or normalized.

Decisions on tooling requirements (Python version, formatter, linter, testing framework, documentation format rules, etc.).

Documentation format structures needed for GPT comprehension and update consistency.

Outputs Expected

A documented, coherent, and ready-to-use foundational environment that supports beginning Phase 1 development immediately.

A clearly defined and ingestible documentation structure (including rules for updates, organization, and GPT usage).

Initial PROJECT_CANON content sufficient to ground early architectural decisions.

High-level folder taxonomy and repository layout standards.

Tooling and environment standards documented and agreed upon.

A uniform set of foundational constraints that future phases must respect unless explicitly revised.

Constraints

Phase 0 introduces no system functionality beyond documentation, standards, and foundational constructs.

All definitions must remain high-level, avoiding premature architectural or implementation decisions.

No module, engine, or functional code may be designed or implemented beyond establishing the environment in which they will later exist.

All documentation and standards produced must be maintainable by GPTs and must align with PROJECT_CANON and GPT_RESOURCE_INDEX.

Phase 0 outputs must not conflict with later roadmap phases; they serve as prerequisites, not prescriptive architecture.

Notes

Phase 0 is intentionally minimal in system ‚Äúbehavior‚Äù but maximal in importance: all later phases assume its rules and structures exist.

This phase acts as the ‚Äúinitiator card‚Äù for the entire system lifecycle‚Äîevery new phase begins from the standards and canon established here.

Some items may be defined conceptually here but materially created during early implementation phases; Phase 0‚Äôs job is to define them clearly enough for consistent execution.

The focus is clarity, consistency, and readiness: the project must be in a state where GPTs can begin producing Phase 1 artifacts without ambiguity.


üß† GPT Instructions (Do Not Delete)

You are a GPT working with Andrew on the LillyCORE project.

Before generating anything, you MUST:

Ask which role you should assume (Architect or Implementer) if unclear.
Verify role, phase, and current goals with Andrew before beginning.
Follow the project‚Äôs rules as defined in the GPT_RESOURCE_INDEX (provided by Andrew when needed).
NEVER invent system constraints, rules, or interpretations.
ALWAYS ask Andrew when something is unclear or underspecified.
When producing output:

Follow the exact Feature Card structure shown in documentation.
Do NOT add additional sections.
Do NOT omit sections.
Do NOT write real code unless you are the Implementer.
If you need Canon/Roadmap/Feature docs, ask Andrew to paste relevant sections.
DOCUMENT INGESTION RULE Before performing ANY reasoning, drafting, questioning, or planning:

You MUST inspect GPT_RESOURCE_INDEX.

For every document listed in it that is not yet present in this conversation, you MUST say:

‚ÄúPlease provide the full content of <DOC_NAME> so I may load it before continuing.‚Äù

You MUST NOT proceed with any task until all such documents have been provided and you confirm you have read and ingested them.

When starting a new phase or returning after long context loss, you MUST repeat this rule.

Current work: You are required to review GPT_RESOURCE_INDEX and any other relevent documents listed therein before begining any work. Andrew will continue to provide relevent parts of it and other supporting documents as needed.

**Phase 1 ‚Äî Core Loop, Logging, User Preferences

Milestone Description**

Purpose

Establish LillyCORE‚Äôs first functional runtime layer‚Äîintroducing a stable execution heartbeat, unified logging behavior, structured error handling, and the foundational preference system that anchors persistent system identity. Phase 1 transitions LillyCORE from ‚Äúpreparation‚Äù to ‚Äúan application that runs,‚Äù enabling all subsequent phases that rely on a predictable runtime environment.

Scope

Implement a functional core runtime loop capable of continuous operation.

Create unified logging infrastructure that all future modules will depend on.

Define and implement functional error envelopes, including propagation and formatting rules.

Introduce the system-wide preference loader with persistence and override capability.

Establish AI Pool definitions (conceptual and structural only), without execution behavior.

Extend Phase-0 standards where necessary (e.g., documenting runtime contracts, logging rules, and error semantics).

System Impact

First appearance of continuous runtime behavior in the system.

Introduces guaranteed logging pathways that later modules will plug into.

Creates a stable identity layer through persisted system preferences, enabling personalized behavior in future phases.

Establishes the conventions for how errors are wrapped, recorded, and surfaced throughout the entire architecture.

Defines the scaffolding for AI pool organization, allowing Phase 2 to attach functional execution logic.

Ensures all future development assumes a reliable loop, predictable logging, and known preference location/format.

Inputs Required

All foundational structures from Phase 0, including doc formats and technical standards.

Roadmap definition for Phase 1.

Any Canon rules governing runtime behavior, identity persistence, and system integrity (as defined so far).

Directory/folder taxonomy that supports logging, prefs, and runtime configuration.

Decisions on:

preferred persistence mechanism for preferences (file, object, or other)

logging channels (console, file, structured output)

error envelope schema

Outputs Expected

A functioning application runtime loop that can start, run, and stop cleanly.

Unified logging system with documented entry points and formatting rules.

Functional error envelope implementation, integrated into the logging and runtime pathways.

Operational system preference loader with persistence, overrides, and canonical storage format.

AI pool structural definitions (types, fields, relationships) prepared for Phase 2‚Äôs execution engine.

Documentation updated to reflect Phase 1 runtime architecture, logging schema, error semantics, and preference rules.

Constraints

No AI execution, scheduling, or chaining yet‚ÄîAI pools are structural only.

No module-level functionality other than what the runtime, logging, and preferences strictly require.

Must not introduce architectural decisions belonging to Phase 2 (AI execution), Phase 3 (data durability engine), or Phase 4+ subsystems.

All behaviors must align with Phase-0 standards and documentation conventions.

Preference system must be minimal but extensible for future module needs.

Core loop must remain intentionally simple‚Äîno advanced orchestration or plugin behavior yet.

Notes

This is the ‚Äúfirst spark‚Äù of LillyCORE: the transition from a designed system to a running one.

The logging and error envelope structures established here will act as substrate for every future subsystem‚Äôs observability.

Phase-1 preferences lay groundwork for user identity models introduced in later phases.

The AI pool definitions act as placeholders whose semantics become real in Phase 2‚Äôs execution layer.

Clarity and stability matter more than completeness: this runtime must be something all later phases can rely on without surprises.

NOTE: Phase 1 MUST include the first real infrastructure setup

During Phase 0, all decisions (lint tools, formatter, CI expectations, directory conventions, etc.) were chosen but not implemented.
Phase 1 must implement all initial project infrastructure, including:

NOTE DIRECTLY FROM ANDREW: somf of this IS actually set up, but i dont know if any of it is correct, so when making phase 1.1 or whenever this is, ensure we chek if it already exsists, if it does make sure it is configured properly (if not fix it) and if it dosn't exsist create it, install it, configuyre it, wahatever. END OF ANDREW NOTE

Repository Initialization

Create the actual Git repository containing the project files.

Establish the initial folder layout as defined in TECH_SPEC.

Add Configuration Files

pyproject.toml (Black, Ruff, version metadata).

Any initial project metadata files required by the spec.

Set Up CI

Configure GitHub Actions (or chosen CI) to run:

Black formatting check.

Ruff lint check.

Test suite once available.

CI must fail on:

Black reformatting needed.

Ruff lint errors.

Test failures.

Install Development Tooling

Add pre-commit hooks (Black + Ruff), or equivalent scripts.

Ensure all Implementer GPT expectations in TECH_SPEC become enforceable.

Create Developer Automation

Any scripts, Makefile targets, or convenience commands chosen in Phase 0.

GPT RULE Activation

Once CI is online, the burden shifts from human reminders to CI enforcement.

Implementer GPTs no longer need to manually remind Andrew to run Black/Ruff locally.

üîí Why this note exists

Because LillyCORE currently consists only of:

The doc folders,

Specs,

Cards,

And no actual repository or automation ‚Äî

‚Äîthis note ensures Phase 1 builds the real foundations and nothing quietly slips through the cracks.

======================================================

üß† GPT Instructions (Do Not Delete)

You are a GPT working with Andrew on the LillyCORE project.

Before generating anything, you MUST:

Ask which role you should assume (Architect or Implementer) if unclear.
Verify role, phase, and current goals with Andrew before beginning.
Follow the project‚Äôs rules as defined in the GPT_RESOURCE_INDEX (provided by Andrew when needed).
NEVER invent system constraints, rules, or interpretations.
ALWAYS ask Andrew when something is unclear or underspecified.
When producing output:

Follow the exact Feature Card structure shown in documentation.
Do NOT add additional sections.
Do NOT omit sections.
Do NOT write real code unless you are the Implementer.
If you need Canon/Roadmap/Feature docs, ask Andrew to paste relevant sections.
DOCUMENT INGESTION RULE Before performing ANY reasoning, drafting, questioning, or planning:

You MUST inspect GPT_RESOURCE_INDEX.

For every document listed in it that is not yet present in this conversation, you MUST say:

‚ÄúPlease provide the full content of <DOC_NAME> so I may load it before continuing.‚Äù

You MUST NOT proceed with any task until all such documents have been provided and you confirm you have read and ingested them.

When starting a new phase or returning after long context loss, you MUST repeat this rule.

Current work: You are required to review GPT_RESOURCE_INDEX and any other relevent documents listed therein before begining any work. Andrew will continue to provide relevent parts of it and other supporting documents as needed.

### Purpose

Establish a robust, model-agnostic AI execution layer that sits beneath all higher-level behavior in LillyCORE. Phase 2 defines and implements the AI pools and their safe execution wrappers so that any compatible LLM backend can be plugged in, while all future system code talks only to pools and never to specific models directly.

### Scope

* Define and implement three concrete AI pools:

  * Conversational pool
  * Deterministic lightweight pool
  * Worker/back-end pool
* Wire all pools to at least one real, locally available LLM backend (it can be the same model/endpoint for all three initially).
* Design and implement execution envelopes that:

  * Accept a request from the core system
  * Route it through the appropriate pool
  * Enforce safety (timeouts, retries, basic resource guarding)
  * Return structured results and error states.
* Provide a unified interface for the rest of LillyCORE to call ‚Äúan AI‚Äù via pools rather than directly targeting a model or provider.
* Integrate the execution framework conceptually with the Phase 1 core loop (APIs and call paths exist), while leaving higher-level orchestration and semantics to later phases.

### System Impact

* Introduces the abstraction boundary between ‚Äúthe system‚Äù and ‚Äúthe AIs‚Äù: from this point on, all higher-level code is written against pools, not raw models.
* Creates a single, consistent execution framework capable of hosting multiple models and backends over time, without rewriting the rest of the system.
* Establishes safety behavior around AI calls (timeouts, retries, failure handling) so that misbehaving models or prompts cannot easily stall or overwhelm the host environment.
* Provides the underlying fabric that later phases (DRIFT_ENGINE, HELPER_ENGINE, plugins, UX) will assume for all AI interactions.
* Begins the separation of concerns between conversational, deterministic/utility, and worker-style AI activities, even if they share a backend initially.

### Inputs Required

* Phase 1 runtime loop and logging/error envelope behavior.
* Knowledge of which local LLM backend(s) are available and how they are invoked (CLI, HTTP, library, etc.).
* Any existing preferences or configuration patterns that should influence pool selection or execution limits (e.g., global AI settings, resource constraints).
* High-level understanding of future uses of each pool (from the roadmap and any relevant design notes) to ensure the pool interfaces are future-proof.
* Decisions about baseline safety policies for execution (maximum runtime, concurrency expectations, failure handling strategy).

### Outputs Expected

* Three implemented AI pools (conversational, deterministic/lightweight, worker/back-end), each with:

  * A defined interface/contract
  * A working connection to at least one local LLM backend
  * Logging and error handling using Phase 1 mechanisms.
* Execution envelopes that encapsulate:

  * Request/response structures
  * Timeouts and retry logic
  * Error wrapping and reporting.
* A documented, stable API for other parts of LillyCORE to send AI work to pools without knowing model details.
* Configuration or preference entries (system-level) that control which backend(s) each pool uses and any key limits (e.g., max tokens, timeout values).
* Updated documentation explaining pool roles, execution behavior, and how future phases should use them.

### Constraints

* No higher-level reasoning orchestration, task typing, or helper semantics are introduced here; those belong to later phases (e.g., DRIFT/HELPER engines).
* All AI interaction from this phase onward must go through pools; direct calls to a specific model/provider are considered out of bounds for new code.
* Safety must be prioritized: execution envelopes must be designed so that misconfiguration or bad prompts cannot easily ‚Äúmelt‚Äù or lock up the machine (e.g., enforce hard timeouts and basic resource guards).
* The design must remain backend-agnostic: swapping or adding an LLM provider should not require changes to callers, only to pool configuration or adapters.
* Integration with the core loop is limited to exposing callable interfaces and basic wiring; complex routing, scheduling, or multi-step flows are deferred.

### Notes

* It is acceptable for all pools to initially point to the same underlying model/backend, as long as their interfaces and responsibilities are clearly separated for future divergence.
* Phase 2 is the critical inflection point where LillyCORE stops thinking in terms of ‚Äúthis model‚Äù and starts thinking in terms of ‚Äúthis kind of AI work goes to this pool.‚Äù
* The execution framework created here will be reused and extended, not replaced, in later phases; getting the contracts and safety behavior right is more important than supporting every advanced feature immediately.
* This phase should leave the system in a state where attaching additional local or remote LLMs is straightforward and does not require revisiting the core loop or main application logic.

üß† GPT Instructions (Do Not Delete)

You are a GPT working with Andrew on the LillyCORE project.

Before generating anything, you MUST:

Ask which role you should assume (Architect or Implementer) if unclear.
Verify role, phase, and current goals with Andrew before beginning.
Follow the project‚Äôs rules as defined in the GPT_RESOURCE_INDEX (provided by Andrew when needed).
NEVER invent system constraints, rules, or interpretations.
ALWAYS ask Andrew when something is unclear or underspecified.
When producing output:

Follow the exact Feature Card structure shown in documentation.
Do NOT add additional sections.
Do NOT omit sections.
Do NOT write real code unless you are the Implementer.
If you need Canon/Roadmap/Feature docs, ask Andrew to paste relevant sections.
DOCUMENT INGESTION RULE Before performing ANY reasoning, drafting, questioning, or planning:

You MUST inspect GPT_RESOURCE_INDEX.

For every document listed in it that is not yet present in this conversation, you MUST say:

‚ÄúPlease provide the full content of <DOC_NAME> so I may load it before continuing.‚Äù

You MUST NOT proceed with any task until all such documents have been provided and you confirm you have read and ingested them.

When starting a new phase or returning after long context loss, you MUST repeat this rule.

Current work: You are required to review GPT_RESOURCE_INDEX and any other relevent documents listed therein before begining any work. Andrew will continue to provide relevent parts of it and other supporting documents as needed.

# Phase 3 ‚Äî Durable Data & Document Layer

Milestone Description

### Purpose

Create a unified, local-only persistence layer that becomes the single, consistent gateway between LillyCORE and the physical disk. Phase 3 establishes the canonical data model skeleton, the document handler, and the rules by which all core system state, documents, and future data domains will be stored, read, and validated. By the end of this phase, the system can reliably remember what it needs across runs using a uniform, secure, and extensible structure.

### Scope

* Define a canonical data model **skeleton** that covers all major future domains (e.g., system/user preferences, runtime/state tracking, documents/notes, plugin data, AI artifacts indexes), even if many of these are only partially implemented now.
* Choose and standardize the core persistence technologies (e.g., database vs files, schema conventions, directory layout) with the intent that they are:

  * Local, offline-first.
  * Suitable for long-term growth.
  * Swappable with minimal disruption if needed later.
* Implement the **initial working subset** of the data layer for what already exists or is immediately needed:

  * System-wide preferences (and any existing config state from earlier phases).
  * State/tracking structures that record what the system is doing and has done (e.g., sessions, runs, key events).
  * A document handler that reads/writes structured documents via this layer rather than directly touching the filesystem.
* Establish the data layer as the **primary in/out path to disk and tables**, with clearly defined exceptions (e.g., system logs, AI sandbox outputs, and future Drift/personal tables).
* Introduce a security/validation step for all data-layer writes, to catch malformed or suspicious payloads before they touch disk or persistent storage.

### System Impact

* Introduces a **single persistence spine** for LillyCORE, decoupling higher-level logic from specific storage mechanisms and file layouts.
* Provides a uniform way to define, evolve, and query data structures, reducing fragmentation and ad-hoc persistence patterns in later phases.
* Enables the system to carry forward important state across runs in a consistent format, supporting more advanced behavior (personalization, history, context) in later phases.
* Establishes the data layer as a security and integrity gate, reducing the risk of corrupted, malformed, or malicious data being written directly to disk.
* Creates a foundation for future multi-user, notes, plugin, and engine-specific data without needing to redesign the core persistence story.

### Inputs Required

* Outputs and contracts from:

  * Phase 1 (runtime loop, logging, error envelopes, system preferences).
  * Phase 2 (AI pool structures and any configuration that needs persistence).
* Roadmap expectations for future phases that will rely on persistent data (e.g., Notes Plugin, Drift Engine, Helper Engine, plugins, multi-user support), to inform the canonical model skeleton.
* Any Canon rules or design decisions regarding:

  * Privacy and security guarantees.
  * Local-only operation and plugin-mediated external access.
* Knowledge of what data is already being produced (preferences, state markers, basic logs, configuration) that should be tracked and/or persisted uniformly.
* Decisions about:

  * The primary storage technology family (e.g., SQL-style vs structured files).
  * How documents and structured data are mapped into schemas, directories, and tables.
  * Which write paths are allowed to bypass the data layer (logs, AI sandbox, specific future tables) and why.

### Outputs Expected

* A documented **canonical data model skeleton** covering:

  * Preferences/config.
  * System/runtime state and tracking.
  * Documents/notes and similar artifacts.
  * Future-facing slots for plugins, engines, and other subsystems.
* A working **data layer component** that:

  * Owns read/write operations to disk and/or database for the initial in-scope data.
  * Exposes clear interfaces for higher-level code to store and retrieve data without touching the filesystem or DB directly.
* A **document handler** that:

  * Reads/writes documents through the data layer.
  * Uses consistent formats and directory/table conventions.
* Initial schemas/tables/directories created and wired up for:

  * Preferences.
  * Core system state/tracking.
  * At least basic document storage used by early-phase features.
* A defined and documented **security/validation step** for writes, specifying:

  * What is checked.
  * What is rejected.
  * How errors are surfaced.
* Updated documentation describing:

  * The role and boundaries of the data layer.
  * The mapping between conceptual entities and their storage.
  * Which components are allowed to write directly to disk and under what conditions.

### Constraints

* The data layer is **local-only** and must function fully offline; any external storage or sync must be introduced later via plugins or other explicit mechanisms.
* The data layer is the **exclusive path** for application-level reads/writes to disk and persistent tables, except for:

  * System logs.
  * AI sandboxing outputs.
  * The specific personal/Drift-related tables planned for later phases, which have their own carefully defined handling.
* Multi-user semantics and complex user-level data models are **not implemented** here; at most, the canonical model and schemas may include placeholders or patterns that anticipate them.
* Technology choices should be:

  * Stable enough for long-term use.
  * Abstracted behind interfaces so that swapping backends later is possible with minimal refactoring.
* Performance tuning is secondary to clarity, safety, and consistency: correctness, uniformity, and extensibility come first.
* No high-level business logic (plugins, engines, UX flows) is implemented in this phase; only the persistence backbone and document handling that those future pieces will rely on.

### Notes

* This phase is where LillyCORE‚Äôs ‚Äúmemory‚Äù stops being informal and becomes a structured, intentional system. Even if not all data domains are fully used yet, the way they will be stored is defined here.
* Not all future tables or directories must be fully implemented now, but the **patterns and schemas** should be consistent and ready to extend as new features arrive.
* The document handler and data layer are tightly linked; all future document-centric features (System DOC, Notes Plugin, etc.) should rely on this foundation rather than introducing new ad-hoc storage paths.
* Clear documentation of the exceptions to the ‚Äúdata layer owns disk‚Äù rule (logs, sandbox, specific personal tables) is important so that later work doesn‚Äôt accidentally bypass safety and consistency.

üß† GPT Instructions (Do Not Delete)

You are a GPT working with Andrew on the LillyCORE project.

Before generating anything, you MUST:

Ask which role you should assume (Architect or Implementer) if unclear.
Verify role, phase, and current goals with Andrew before beginning.
Follow the project‚Äôs rules as defined in the GPT_RESOURCE_INDEX (provided by Andrew when needed).
NEVER invent system constraints, rules, or interpretations.
ALWAYS ask Andrew when something is unclear or underspecified.
When producing output:

Follow the exact Feature Card structure shown in documentation.
Do NOT add additional sections.
Do NOT omit sections.
Do NOT write real code unless you are the Implementer.
If you need Canon/Roadmap/Feature docs, ask Andrew to paste relevant sections.
DOCUMENT INGESTION RULE Before performing ANY reasoning, drafting, questioning, or planning:

You MUST inspect GPT_RESOURCE_INDEX.

For every document listed in it that is not yet present in this conversation, you MUST say:

‚ÄúPlease provide the full content of <DOC_NAME> so I may load it before continuing.‚Äù

You MUST NOT proceed with any task until all such documents have been provided and you confirm you have read and ingested them.

When starting a new phase or returning after long context loss, you MUST repeat this rule.

Current work: You are required to review GPT_RESOURCE_INDEX and any other relevent documents listed therein before begining any work. Andrew will continue to provide relevent parts of it and other supporting documents as needed.

Phase 4 ‚Äî Notes Plugin V0

Milestone Description

Purpose

Create the first minimal plugin built on top of the existing document/data layer so LillyCORE has a cheap, low-risk testbed for ‚Äúmemory flow.‚Äù Phase 4 introduces a simple Notes Plugin that can receive transcript-like content, pass it through the current system stack, and emit user-facing artifacts (e.g., summary files) without requiring mature querying, UX, or full DRIFT/HELPER behavior.

Scope

Define a minimal Notes Plugin V0 that:

Accepts transcript-style input (real or dummy).

Produces basic summarized or processed output (dummy summaries are acceptable).

Accumulates notes or outputs over time in a simple, traceable way.

Integrate the plugin with the existing System DOC / document layer from earlier phases so that it:

Uses established structures and conventions for content where appropriate.

Does not bypass the durable data/document layer for any system-owned data.

Introduce a user output path for plugin results:

Define and use a ‚Äúuser output‚Äù folder or equivalent location on disk.

Ensure plugin output is stored as user-visible files (e.g., text/markdown), not as core database tables.

Provide a minimal harness or mechanism to invoke this plugin as part of normal system operation (e.g., via the runtime or a simple call path), using only the systems that currently exist by this phase.

Use the Notes Plugin as a lightweight testbed for exercising:

The data/document layer read/write behavior.

The AI pool execution framework (once connected in later phases), starting with dummy behavior here.

System Impact

Introduces LillyCORE‚Äôs first real plugin-like component, even if the full Plugin Engine appears in a later phase.

Establishes a concrete example of how system components can:

Take structured or semi-structured input.

Pass it through the system stack.

Emit user-facing artifacts in a controlled, repeatable way.

Validates the durability/document layer by routing real use-cases (notes/transcripts) through it, helping uncover gaps in schema, structure, or write-path rules from earlier phases.

Provides a low-stakes environment for experimenting with memory flow and later DRIFT/HELPER integrations, without entangling those engines directly at this stage.

Starts to shape expectations for how future plugins will interact with storage, output folders, and system boundaries.

Inputs Required

Outputs and contracts from:

Phase 1 (runtime loop, logging, error handling).

Phase 2 (AI pools and execution framework, even if used minimally or in stub form here).

Phase 3 (durable data/document layer and any System DOC structures that exist).

Knowledge of:

Where user-visible outputs should live on disk (user output folder concept).

What minimal metadata or structure is needed to keep plugin outputs traceable (e.g., timestamps, source IDs, run IDs).

Any relevant Canon rules regarding:

Plugin boundaries vs core system.

What counts as ‚Äúuser output‚Äù vs ‚Äúsystem-internal state.‚Äù

Clarified expectations for:

How much of the transcript/notes pipeline needs to be represented now vs left as future behavior.

How the plugin should be triggered during development/testing (e.g., manual calls, simple runtime hooks).

Outputs Expected

A defined Notes Plugin V0 with:

A minimal interface for receiving transcript-like content.

A simple processing path that can generate dummy or basic summaries.

A way to accumulate and store resulting ‚Äúnotes‚Äù or summaries over time.

A user output folder (or equivalent) plus rules for how plugin output files are:

Named.

Organized.

Related back to their source inputs or sessions.

A thin integration between the plugin and:

The document/data layer for any system-owned content it touches.

The runtime/logging stack for visibility into when and how it runs.

Documentation that describes:

The role and limitations of Notes Plugin V0.

How it uses the storage and document layer.

How other phases can safely rely on it for testing memory flow.

Constraints

Notes Plugin V0 is deliberately minimal and disposable: it must be useful for testing, but not so entangled that the system depends on it for core functionality.

No advanced querying is required at this phase:

Rich search, semantic retrieval, or complex query interfaces are out of scope.

Simple listing or inspection of outputs (via file system or basic tooling) is sufficient.

No direct DRIFT/HELPER logic is implemented here:

The plugin may produce data that future engines can use,

But it does not simulate or implement their behavior.

All system-level data interactions still respect the durable data/document layer rules; only user-facing outputs are written directly as files in the user output location.

UX and frontend layers are out of scope; interaction can be backend-only or developer/test-oriented.

The phase should focus on small, end-to-end testable steps that exercise the current stack, rather than polishing or expanding plugin capabilities.

Notes

Think of Notes Plugin V0 as a ‚Äúlab bench‚Äù for LillyCORE: it grows over time, mirrors the system‚Äôs increasing capabilities, and remains a safe place to test how memory and documents move through the stack.

The plugin is expected to start extremely simple‚Äîpossibly generating dummy summaries‚Äîand only gradually become more intelligent as later phases introduce richer AI behavior and engines.

By keeping this plugin thin, backend-focused, and grounded in the existing storage/doc layer, Phase 4 avoids re-inventing infrastructure while still providing a concrete way to observe and test memory flow end-to-end.

